{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "- Deleting punctuations except ! - bcos can show emotion\n",
    "- Removing numbers - no use\n",
    "- Remove empty rows\n",
    "- Remove emoji - chatbot won't allow emoji\n",
    "- Remove more than one space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    30000 non-null  object\n",
      " 1   class   30000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 468.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = 'Suicide_Detection.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "df = df.head(30000)\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text        class\n",
      "0  Ex Wife Threatening SuicideRecently I left my ...      suicide\n",
      "1  Am I weird I dont get affected by compliments ...  non-suicide\n",
      "2  Finally is almost over So I can never hear  ha...  non-suicide\n",
      "3          i need helpjust help me im crying so hard      suicide\n",
      "4  Im so lostHello my name is Adam  and Ive been ...      suicide\n"
     ]
    }
   ],
   "source": [
    "def pre_processing(data):\n",
    "    data = data.apply(lambda x: ''.join([char for char in x if not char.isdigit()]))\n",
    "    data = data.apply(lambda x: \" \".join(x.split()))\n",
    "    data = data.apply(lambda x: re.sub(r'[^\\w\\s!]', '', x))\n",
    "    data = data.apply(remove_emoji)\n",
    "\n",
    "    return data\n",
    "\n",
    "df['text'] = pre_processing(df['text'])\n",
    "\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  class\n",
      "0  Ex Wife Threatening SuicideRecently I left my ...      1\n",
      "1  Am I weird I dont get affected by compliments ...      0\n",
      "2  Finally is almost over So I can never hear  ha...      0\n",
      "3          i need helpjust help me im crying so hard      1\n",
      "4  Im so lostHello my name is Adam  and Ive been ...      1\n"
     ]
    }
   ],
   "source": [
    "#Map output to be binary 1 or 0\n",
    "mapping = {'suicide': 1, 'non-suicide': 0}\n",
    "df['class'] = df['class'].map(mapping)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model - classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    2    29     4 ...    10   171   402]\n",
      " [   49     2   451 ...     0     0     0]\n",
      " [  289    15   275 ...     0     0     0]\n",
      " ...\n",
      " [  393  1059  9316 ...    97    34   108]\n",
      " [   18 10053     9 ...    15   352     9]\n",
      " [  800     1    65 ...     0     0     0]]\n",
      "[1 0 0 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 50\n",
    "\n",
    "X = df['text'].values\n",
    "y = df['class'].values\n",
    "y=np.array(y)\n",
    "\n",
    "\n",
    "# # Tokenize the entire dataset\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "# Convert text to sequences of integers\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "X = pad_sequences(sequences, maxlen=maxlen, padding=\"post\")\n",
    "\n",
    "print(X)\n",
    "print(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 16  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 16  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.35)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 50)]              0         \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, 50, 16)            320800    \n",
      " ng (TokenAndPositionEmbedd                                      \n",
      " ing)                                                            \n",
      "                                                                 \n",
      " transformer_block (Transfo  (None, 50, 16)            2768      \n",
      " rmerBlock)                                                      \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 16)                0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 20)                340       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 20)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 42        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 323950 (1.24 MB)\n",
      "Trainable params: 323950 (1.24 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "525/525 [==============================] - 16s 25ms/step - loss: 0.3935 - accuracy: 0.8382 - val_loss: 0.2776 - val_accuracy: 0.8868\n",
      "Epoch 2/20\n",
      "525/525 [==============================] - 13s 25ms/step - loss: 0.2381 - accuracy: 0.9118 - val_loss: 0.2804 - val_accuracy: 0.8958\n",
      "Epoch 3/20\n",
      "525/525 [==============================] - 12s 22ms/step - loss: 0.1888 - accuracy: 0.9329 - val_loss: 0.3836 - val_accuracy: 0.8700\n",
      "Epoch 4/20\n",
      "525/525 [==============================] - 10s 20ms/step - loss: 0.1547 - accuracy: 0.9467 - val_loss: 0.3090 - val_accuracy: 0.8903\n",
      "Epoch 5/20\n",
      "525/525 [==============================] - 9s 17ms/step - loss: 0.1248 - accuracy: 0.9561 - val_loss: 0.3452 - val_accuracy: 0.8899\n",
      "Epoch 6/20\n",
      "525/525 [==============================] - 9s 18ms/step - loss: 0.1069 - accuracy: 0.9636 - val_loss: 0.3751 - val_accuracy: 0.8889\n",
      "Epoch 7/20\n",
      "525/525 [==============================] - 13s 25ms/step - loss: 0.0876 - accuracy: 0.9701 - val_loss: 0.4463 - val_accuracy: 0.8776\n",
      "Epoch 8/20\n",
      "525/525 [==============================] - 14s 27ms/step - loss: 0.0706 - accuracy: 0.9771 - val_loss: 0.4983 - val_accuracy: 0.8824\n",
      "Epoch 9/20\n",
      "525/525 [==============================] - 10s 20ms/step - loss: 0.0544 - accuracy: 0.9818 - val_loss: 0.5635 - val_accuracy: 0.8739\n",
      "Epoch 10/20\n",
      "525/525 [==============================] - 9s 17ms/step - loss: 0.0483 - accuracy: 0.9829 - val_loss: 0.6462 - val_accuracy: 0.8750\n",
      "Epoch 11/20\n",
      "525/525 [==============================] - 9s 17ms/step - loss: 0.0444 - accuracy: 0.9841 - val_loss: 0.6427 - val_accuracy: 0.8611\n",
      "Epoch 12/20\n",
      "525/525 [==============================] - 8s 15ms/step - loss: 0.0388 - accuracy: 0.9870 - val_loss: 0.7914 - val_accuracy: 0.8682\n",
      "Epoch 13/20\n",
      "525/525 [==============================] - 9s 17ms/step - loss: 0.0309 - accuracy: 0.9891 - val_loss: 0.7959 - val_accuracy: 0.8653\n",
      "Epoch 14/20\n",
      "525/525 [==============================] - 9s 17ms/step - loss: 0.0277 - accuracy: 0.9905 - val_loss: 0.8609 - val_accuracy: 0.8654\n",
      "Epoch 15/20\n",
      "525/525 [==============================] - 9s 18ms/step - loss: 0.0249 - accuracy: 0.9915 - val_loss: 0.8563 - val_accuracy: 0.8621\n",
      "Epoch 16/20\n",
      "525/525 [==============================] - 10s 19ms/step - loss: 0.0250 - accuracy: 0.9921 - val_loss: 0.7771 - val_accuracy: 0.8560\n",
      "Epoch 17/20\n",
      "525/525 [==============================] - 11s 20ms/step - loss: 0.0222 - accuracy: 0.9918 - val_loss: 0.9854 - val_accuracy: 0.8633\n",
      "Epoch 18/20\n",
      "525/525 [==============================] - 12s 22ms/step - loss: 0.0252 - accuracy: 0.9916 - val_loss: 0.9508 - val_accuracy: 0.8554\n",
      "Epoch 19/20\n",
      "525/525 [==============================] - 9s 17ms/step - loss: 0.0198 - accuracy: 0.9937 - val_loss: 0.9454 - val_accuracy: 0.8660\n",
      "Epoch 20/20\n",
      "525/525 [==============================] - 9s 16ms/step - loss: 0.0171 - accuracy: 0.9939 - val_loss: 1.1479 - val_accuracy: 0.8606\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 1.1658 - accuracy: 0.8535\n",
      "Test Loss: 1.1658167839050293\n",
      "Test Accuracy: 0.8535000085830688\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_val = np.array(X_val)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(\n",
    "    X_train, y_train, batch_size=32, epochs=20, validation_data=(X_val, y_val)\n",
    ")\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 166ms/step\n",
      "[[0.36016306 0.63983697]]\n",
      "Predicted Class: suicidal\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"cheeseburger sucks\"\n",
    "text = pre_processing(pd.Series(text))\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(text)\n",
    "\n",
    "text = pad_sequences(sequences, maxlen=maxlen, padding=\"post\")\n",
    "predictions = model.predict(text)\n",
    "print(predictions)\n",
    "predicted_class = \"non-suicidal\" if predictions[0][0] > 0.65 else \"suicidal\"\n",
    "print(f\"Predicted Class: {predicted_class}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
