{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "- Deleting punctuations except ! - bcos can show emotion\n",
    "- Removing numbers - no use\n",
    "- Remove empty rows\n",
    "- Remove emoji - chatbot won't allow emoji\n",
    "- Remove more than one space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    30000 non-null  object\n",
      " 1   class   30000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 468.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = 'Suicide_Detection.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "df = df.head(30000)\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text        class\n",
      "0  Ex Wife Threatening SuicideRecently I left my ...      suicide\n",
      "1  Am I weird I dont get affected by compliments ...  non-suicide\n",
      "2  Finally is almost over So I can never hear  ha...  non-suicide\n",
      "3          i need helpjust help me im crying so hard      suicide\n",
      "4  Im so lostHello my name is Adam  and Ive been ...      suicide\n"
     ]
    }
   ],
   "source": [
    "def pre_processing(data):\n",
    "    data = data.apply(lambda x: ''.join([char for char in x if not char.isdigit()]))\n",
    "    data = data.apply(lambda x: \" \".join(x.split()))\n",
    "    data = data.apply(lambda x: re.sub(r'[^\\w\\s!]', '', x))\n",
    "    data = data.apply(remove_emoji)\n",
    "\n",
    "    return data\n",
    "\n",
    "df['text'] = pre_processing(df['text'])\n",
    "\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  class\n",
      "0  Ex Wife Threatening SuicideRecently I left my ...      1\n",
      "1  Am I weird I dont get affected by compliments ...      0\n",
      "2  Finally is almost over So I can never hear  ha...      0\n",
      "3          i need helpjust help me im crying so hard      1\n",
      "4  Im so lostHello my name is Adam  and Ive been ...      1\n"
     ]
    }
   ],
   "source": [
    "#Map output to be binary 1 or 0\n",
    "mapping = {'suicide': 1, 'non-suicide': 0}\n",
    "df['class'] = df['class'].map(mapping)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model - classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    2    29     4 ...    10   171   402]\n",
      " [   49     2   451 ...     0     0     0]\n",
      " [  289    15   275 ...     0     0     0]\n",
      " ...\n",
      " [  393  1059  9316 ...    97    34   108]\n",
      " [   18 10053     9 ...    15   352     9]\n",
      " [  800     1    65 ...     0     0     0]]\n",
      "[1 0 0 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 50\n",
    "\n",
    "X = df['text'].values\n",
    "y = df['class'].values\n",
    "y=np.array(y)\n",
    "\n",
    "\n",
    "# # Tokenize the entire dataset\n",
    "# tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "# tokenizer.fit_on_texts(X)\n",
    "\n",
    "# Convert text to sequences of integers\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "X = pad_sequences(sequences, maxlen=maxlen, padding=\"post\")\n",
    "\n",
    "print(X)\n",
    "print(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 8  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 16  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.35)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 50)]              0         \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, 50, 8)             160400    \n",
      " ng_3 (TokenAndPositionEmbe                                      \n",
      " dding)                                                          \n",
      "                                                                 \n",
      " transformer_block_3 (Trans  (None, 50, 8)             880       \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " global_average_pooling1d_3  (None, 8)                 0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 20)                180       \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 20)                0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 2)                 42        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 161502 (630.87 KB)\n",
      "Trainable params: 161502 (630.87 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "525/525 [==============================] - 10s 13ms/step - loss: 0.4444 - accuracy: 0.8030 - val_loss: 0.3329 - val_accuracy: 0.8614\n",
      "Epoch 2/10\n",
      "525/525 [==============================] - 6s 12ms/step - loss: 0.2571 - accuracy: 0.9045 - val_loss: 0.2646 - val_accuracy: 0.8981\n",
      "Epoch 3/10\n",
      "525/525 [==============================] - 6s 11ms/step - loss: 0.1956 - accuracy: 0.9301 - val_loss: 0.2769 - val_accuracy: 0.8972\n",
      "Epoch 4/10\n",
      "525/525 [==============================] - 6s 12ms/step - loss: 0.1602 - accuracy: 0.9452 - val_loss: 0.2839 - val_accuracy: 0.8974\n",
      "Epoch 5/10\n",
      "525/525 [==============================] - 6s 12ms/step - loss: 0.1307 - accuracy: 0.9538 - val_loss: 0.3406 - val_accuracy: 0.8847\n",
      "Epoch 6/10\n",
      "525/525 [==============================] - 6s 11ms/step - loss: 0.1166 - accuracy: 0.9609 - val_loss: 0.3404 - val_accuracy: 0.8933\n",
      "Epoch 7/10\n",
      "525/525 [==============================] - 6s 11ms/step - loss: 0.0979 - accuracy: 0.9678 - val_loss: 0.3933 - val_accuracy: 0.8890\n",
      "Epoch 8/10\n",
      "525/525 [==============================] - 6s 11ms/step - loss: 0.0850 - accuracy: 0.9729 - val_loss: 0.4609 - val_accuracy: 0.8860\n",
      "Epoch 9/10\n",
      "525/525 [==============================] - 6s 11ms/step - loss: 0.0742 - accuracy: 0.9750 - val_loss: 0.4750 - val_accuracy: 0.8779\n",
      "Epoch 10/10\n",
      "525/525 [==============================] - 7s 13ms/step - loss: 0.0638 - accuracy: 0.9797 - val_loss: 0.5479 - val_accuracy: 0.8735\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.5609 - accuracy: 0.8705\n",
      "Test Loss: 0.5609389543533325\n",
      "Test Accuracy: 0.8705000281333923\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_val = np.array(X_val)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(\n",
    "    X_train, y_train, batch_size=32, epochs=10, validation_data=(X_val, y_val)\n",
    ")\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 324ms/step\n",
      "[[0.76624835 0.23375161]]\n",
      "Predicted Class: non-suicidal\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"cheeseburger sucks\"\n",
    "text = pre_processing(pd.Series(text))\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(text)\n",
    "\n",
    "text = pad_sequences(sequences, maxlen=maxlen, padding=\"post\")\n",
    "predictions = model.predict(text)\n",
    "print(predictions)\n",
    "predicted_class = \"non-suicidal\" if predictions[0][0] > 0.65 else \"suicidal\"\n",
    "print(f\"Predicted Class: {predicted_class}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
