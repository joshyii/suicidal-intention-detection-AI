{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with Transformer Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text feeling-bad depressed  \\\n",
      "0  When I was in high school a few years back, I ...         yes        no   \n",
      "1  Nine years ago I was diagnosed with depression...         yes       yes   \n",
      "2  Some background information: My GF of almost 3...         yes       yes   \n",
      "3  My girlfriend ,of about 3 months now ,has been...         yes       yes   \n",
      "4  I'm alway feeling like this. It doesn't even m...         yes       yes   \n",
      "\n",
      "  tired little_interest unusual_behaviour unusual_appetite want_to_harm  \\\n",
      "0   yes             yes                no               no           no   \n",
      "1    no              no               yes               no           no   \n",
      "2   yes             yes                no               no          yes   \n",
      "3    no              no                no               no          yes   \n",
      "4    no              no                no               no           no   \n",
      "\n",
      "  cannot_concentrate cannot_fall_asleep  \n",
      "0                 no                 no  \n",
      "1                 no                 no  \n",
      "2                 no                 no  \n",
      "3                 no                 no  \n",
      "4                 no                 no  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    " \n",
    "f = open('primate_dataset.json')\n",
    " \n",
    "data = json.load(f)\n",
    "X = [d['post_text'] for d in data if 'post_text' in d]\n",
    "data_pre = {}\n",
    "data_pre['text'] = X\n",
    "y_pre = [d['annotations'] for d in data if 'annotations' in d]\n",
    "\n",
    "\n",
    "for i in range(9):\n",
    "    temp =[]\n",
    "    for j in range(len(y_pre)):\n",
    "        temp.append(y_pre[j][i][1])\n",
    "    switcher = {\n",
    "        0: 'feeling-bad',\n",
    "        1: 'depressed',\n",
    "        2: 'tired',\n",
    "        3: 'little_interest',\n",
    "        4: 'unusual_behaviour',\n",
    "        5: 'unusual_appetite',\n",
    "        6: 'want_to_harm',\n",
    "        7: 'cannot_concentrate',\n",
    "        8: 'cannot_fall_asleep'\n",
    "    }\n",
    "    data_pre[switcher.get(i)] = np.array(temp)\n",
    "\n",
    "df = pd.DataFrame(data_pre)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 133    2 2510 ... 1099  116   60]\n",
      " [2815   14    6 ...   62 1392   10]\n",
      " [  25   10   14 ...  162  227  347]\n",
      " ...\n",
      " [ 432   69   32 ...    0    0    0]\n",
      " [ 456   40    3 ...    0    0    0]\n",
      " [  89 3903    4 ...    0    0    0]]\n",
      "[1 1 1 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 300  # Only consider the first 200 words of each movie review\n",
    "\n",
    "X = df['text'].values\n",
    "y = df['feeling-bad'].values\n",
    "mapping_dict = {'yes': 1, 'no': 0}\n",
    "y = [mapping_dict[i] for i in y]\n",
    "y=np.array(y)\n",
    "\n",
    "\n",
    "# Tokenize the entire dataset\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "# Convert text to sequences of integers\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "X = pad_sequences(sequences, maxlen=maxlen, padding=\"post\")\n",
    "\n",
    "print(X)\n",
    "print(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 300)]             0         \n",
      "                                                                 \n",
      " token_and_position_embeddin  (None, 300, 32)          649600    \n",
      " g_4 (TokenAndPositionEmbedd                                     \n",
      " ing)                                                            \n",
      "                                                                 \n",
      " transformer_block_4 (Transf  (None, 300, 32)          10656     \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " global_average_pooling1d_4   (None, 32)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 20)                660       \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 20)                0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 2)                 42        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 660,958\n",
      "Trainable params: 660,958\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "51/51 [==============================] - 13s 213ms/step - loss: 0.3938 - accuracy: 0.8371 - val_loss: 0.4397 - val_accuracy: 0.8429\n",
      "Epoch 2/100\n",
      "51/51 [==============================] - 11s 214ms/step - loss: 0.3419 - accuracy: 0.8546 - val_loss: 0.4433 - val_accuracy: 0.8479\n",
      "Epoch 3/100\n",
      "51/51 [==============================] - 11s 218ms/step - loss: 0.2480 - accuracy: 0.9026 - val_loss: 0.4937 - val_accuracy: 0.7731\n",
      "Epoch 4/100\n",
      "51/51 [==============================] - 10s 201ms/step - loss: 0.1429 - accuracy: 0.9376 - val_loss: 0.5843 - val_accuracy: 0.8055\n",
      "Epoch 5/100\n",
      "51/51 [==============================] - 10s 204ms/step - loss: 0.0551 - accuracy: 0.9856 - val_loss: 0.9252 - val_accuracy: 0.8479\n",
      "Epoch 6/100\n",
      "51/51 [==============================] - 10s 203ms/step - loss: 0.0795 - accuracy: 0.9719 - val_loss: 0.8710 - val_accuracy: 0.8429\n",
      "Epoch 7/100\n",
      "51/51 [==============================] - 10s 205ms/step - loss: 0.0191 - accuracy: 0.9944 - val_loss: 1.0038 - val_accuracy: 0.7382\n",
      "Epoch 8/100\n",
      "51/51 [==============================] - 10s 199ms/step - loss: 0.0563 - accuracy: 0.9825 - val_loss: 0.8857 - val_accuracy: 0.8204\n",
      "Epoch 9/100\n",
      "51/51 [==============================] - 10s 204ms/step - loss: 0.0068 - accuracy: 0.9994 - val_loss: 0.9634 - val_accuracy: 0.8080\n",
      "Epoch 10/100\n",
      "51/51 [==============================] - 10s 199ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.0996 - val_accuracy: 0.8229\n",
      "Epoch 11/100\n",
      "51/51 [==============================] - 10s 203ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.1161 - val_accuracy: 0.8080\n",
      "Epoch 12/100\n",
      "25/51 [=============>................] - ETA: 5s - loss: 7.7138e-04 - accuracy: 1.0000"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(\n",
    "    X_train, y_train, batch_size=32, epochs=100, validation_data=(X_test, y_test)\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
